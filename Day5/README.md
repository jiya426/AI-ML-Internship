
# ğŸŒ³ Decision Trees and Random Forests

### AI & ML Internship â€“ Task 5

## ğŸ“Œ Project Overview

This project focuses on implementing **tree-based machine learning models** including:

* Decision Tree Classifier
* Random Forest Classifier

The objective is to understand how tree-based algorithms work, analyze overfitting, compare model performance, and interpret feature importance.

---

## ğŸ¯ Objectives

* Train a Decision Tree model
* Visualize the decision tree
* Control overfitting using tree depth
* Train a Random Forest model
* Compare performance of both models
* Interpret feature importance
* Evaluate models using cross-validation

---

## ğŸ› ï¸ Tools & Technologies

* Python
* Pandas
* NumPy
* Scikit-learn
* Matplotlib
* Graphviz
* Jupyter Notebook

---

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ Task5.ipynb          # Implementation notebook
â”œâ”€â”€ dataset.csv          # Dataset used (e.g., Heart Disease Dataset)
â”œâ”€â”€ task 5.pdf           # Internship task instructions
â”œâ”€â”€ README.md            # Project documentation
```

---

## âš™ï¸ Implementation Steps

1. Imported and preprocessed the dataset
2. Performed train-test split
3. Trained Decision Tree Classifier
4. Visualized decision tree using Graphviz
5. Analyzed overfitting by adjusting `max_depth`
6. Trained Random Forest Classifier
7. Compared model accuracy
8. Interpreted feature importance
9. Evaluated using cross-validation

---

## ğŸ“Š Model Evaluation

Models were evaluated using:

* Accuracy Score
* Confusion Matrix
* Cross-validation Score

### ğŸ” Observations

* Decision Trees can overfit if depth is not controlled.
* Random Forest improves performance by reducing variance using bagging.
* Feature importance helps identify the most influential variables.

---

## ğŸ“˜ Concepts Covered

* Entropy
* Information Gain
* Overfitting & Underfitting
* Bagging
* Ensemble Learning
* Feature Importance

---

## ğŸš€ How to Run

1. Clone the repository:

```bash
git clone https://github.com/your-username/decision-tree-random-forest.git
```

2. Install required libraries:

```bash
pip install -r requirements.txt
```

3. Open Jupyter Notebook:

```bash
jupyter notebook
```

4. Run `Task5.ipynb`

---

## ğŸ“ˆ What I Learned

* How decision trees split data using entropy and information gain
* How to control model complexity
* Why Random Forest performs better than a single tree
* How ensemble learning reduces overfitting

---

## ğŸ‘©â€ğŸ’» Author

**Jiya Jain**
B.Tech Computer Science
AI/ML & Data Science Intern
